# Language Modelling 

## Overview
We include different notebooks to provide the student with practical means to explore language models and transformers, as well as their application in NLP tasks like wotd-sense disambiguation. We follow an incremental approach in accordance with the contents of session 6 of the course, focused on semantics.

## N-gram based language models
In [this notebook](https://github.com/acastellanos-ie/NLP-SAMBD-EN-2021/blob/main/language_modelling/language%20modelling.ipynb) we are going to start playing with languages models. In particular, we are going to start with the simplest approach based on n-grams. Then, in the following threads, we will move to more advanced approaches based on LSTM and Transformer architectures.

## ELMo

In [this other notebook](https://github.com/acastellanos-ie/NLP-SAMBD-EN-2021/blob/main/language_modelling/language%20modelling_elmo.ipynb) we can review an implementation of a sentiment analyzer leveraging the representation provided by ELMo.

## Fine-tuning pre-trained transformers for text classification
Fine-tuning pre-trained language models learnt with transformers has improved the state of the art in multiple NLP evaluation tasks. In [this notebook](https://github.com/hybridnlp/tutorial/blob/master/01a_nlm_and_contextual_embeddings.ipynb) we fine tune a pre-trained  BERT language model to carry out a binary classification task where tweets are labelled as generated by bots or hurmans.

## Using transformers
In [this notebook]() we finally apply BERT for text generation.


